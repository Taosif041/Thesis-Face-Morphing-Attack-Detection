{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8271362,"sourceType":"datasetVersion","datasetId":4910963},{"sourceId":8290075,"sourceType":"datasetVersion","datasetId":4924493},{"sourceId":8469777,"sourceType":"datasetVersion","datasetId":5050179},{"sourceId":8514818,"sourceType":"datasetVersion","datasetId":5083445},{"sourceId":8526502,"sourceType":"datasetVersion","datasetId":5091679}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nfrom tqdm import tqdm\nfrom transformers import AutoFeatureExtractor, ResNetForImageClassification\nfrom sklearn.metrics import roc_curve\nimport numpy as np\nimport pandas as pd\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T16:25:02.030061Z","iopub.execute_input":"2024-05-29T16:25:02.031147Z","iopub.status.idle":"2024-05-29T16:25:02.038675Z","shell.execute_reply.started":"2024-05-29T16:25:02.031102Z","shell.execute_reply":"2024-05-29T16:25:02.037527Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# # Set the paths to your training, validation, and test directories\n# train_dir = '/kaggle/input/small-dataset/train'\n# val_dir = '/kaggle/input/small-dataset/val'\n# test_dir = '/kaggle/input/small-dataset/test'\n\n# # For the evaluation datasets\n# fm_dir = '/kaggle/input/mad-benchmark-small/FaceMorpher'\n# mg1_dir = '/kaggle/input/mad-benchmark-small/MIPGAN_I'\n# mg2_dir = '/kaggle/input/mad-benchmark-small/MIPGAN_II'\n# oc_dir = '/kaggle/input/mad-benchmark-small/OpenCV'\n# wm_dir = '/kaggle/input/mad-benchmark-small/Webmorph'\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:25:02.041858Z","iopub.execute_input":"2024-05-29T16:25:02.042248Z","iopub.status.idle":"2024-05-29T16:25:02.059133Z","shell.execute_reply.started":"2024-05-29T16:25:02.042213Z","shell.execute_reply":"2024-05-29T16:25:02.058090Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Set the paths to your training, validation, and test directories\ntrain_dir = '/kaggle/input/morph-balanced/train'\nval_dir = '/kaggle/input/morph-balanced/val'\n# test_dir = '/kaggle/input/morph-splitted/test'\n\n# For the evaluation datasets\nfm_dir = '/kaggle/input/mad-benchmark/FaceMorpher'\nmg1_dir = '/kaggle/input/mad-benchmark/MIPGAN_I'\nmg2_dir = '/kaggle/input/mad-benchmark/MIPGAN_II'\noc_dir = '/kaggle/input/mad-benchmark/OpenCV'\nwm_dir = '/kaggle/input/mad-benchmark/Webmorph'\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:25:02.062413Z","iopub.execute_input":"2024-05-29T16:25:02.063110Z","iopub.status.idle":"2024-05-29T16:25:02.069380Z","shell.execute_reply.started":"2024-05-29T16:25:02.063075Z","shell.execute_reply":"2024-05-29T16:25:02.068637Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"\n# Set device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to reverse the class labels\ndef reverse_labels(label):\n    return 1 - label\n\n# Define transformations\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n\nval_test_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n\n\n# Create datasets with reversed labels\ntrain_dataset = datasets.ImageFolder(train_dir, transform=train_transform, target_transform=reverse_labels)\nval_dataset = datasets.ImageFolder(val_dir, transform=val_test_transform, target_transform=reverse_labels)\nfm_dataset = datasets.ImageFolder(fm_dir, transform=val_test_transform, target_transform=reverse_labels)\nmg1_dataset = datasets.ImageFolder(mg1_dir, transform=val_test_transform, target_transform=reverse_labels)\nmg2_dataset = datasets.ImageFolder(mg2_dir, transform=val_test_transform, target_transform=reverse_labels)\noc_dataset = datasets.ImageFolder(oc_dir, transform=val_test_transform, target_transform=reverse_labels)\nwm_dataset = datasets.ImageFolder(wm_dir, transform=val_test_transform, target_transform=reverse_labels)\n\n# Create data loaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\nfm_loader = DataLoader(fm_dataset, batch_size=batch_size, shuffle=False)\nmg1_loader = DataLoader(mg1_dataset, batch_size=batch_size, shuffle=False)\nmg2_loader = DataLoader(mg2_dataset, batch_size=batch_size, shuffle=False)\noc_loader = DataLoader(oc_dataset, batch_size=batch_size, shuffle=False)\nwm_loader = DataLoader(wm_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:25:02.070421Z","iopub.execute_input":"2024-05-29T16:25:02.071070Z","iopub.status.idle":"2024-05-29T16:25:51.204720Z","shell.execute_reply.started":"2024-05-29T16:25:02.071046Z","shell.execute_reply":"2024-05-29T16:25:51.203806Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Define a self-attention layer\nclass SelfAttention(nn.Module):\n    def __init__(self, in_dim):\n        super(SelfAttention, self).__init__()\n        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        batch_size, C, width, height = x.size()\n        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n        energy = torch.bmm(proj_query, proj_key)\n        attention = nn.Softmax(dim=-1)(energy)\n        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(batch_size, C, width, height)\n        out = self.gamma * out + x\n        return out\n\n# Load pre-trained ResNet-50 model\nbase_model = models.resnet50(pretrained=True)\n\n# Insert the attention layer before the final classifier\nclass ResNetWithAttention(nn.Module):\n    def __init__(self, base_model):\n        super(ResNetWithAttention, self).__init__()\n        self.base_model = nn.Sequential(*list(base_model.children())[:-2])  # Exclude the final linear layer\n        self.attention = SelfAttention(in_dim=2048)  # Attention layer with 2048 input channels\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Linear(2048, 1)\n\n    def forward(self, x):\n        x = self.base_model(x)\n        x = self.attention(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\nmodel_with_attention = ResNetWithAttention(base_model)\nmodel_with_attention.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model_with_attention.parameters(), lr=0.001)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:25:51.207464Z","iopub.execute_input":"2024-05-29T16:25:51.208385Z","iopub.status.idle":"2024-05-29T16:25:51.815171Z","shell.execute_reply.started":"2024-05-29T16:25:51.208345Z","shell.execute_reply":"2024-05-29T16:25:51.814359Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training loop\nnum_epochs = 10\nbest_model_wts = model_with_attention.state_dict()\nbest_acc = 0.0\ndataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs}')\n    print('-' * 10)\n\n    for phase in ['train', 'val']:\n        if phase == 'train':\n            model_with_attention.train()  # Set model to training mode\n        else:\n            model_with_attention.eval()  # Set model to evaluate mode\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        with tqdm(total=len(train_loader if phase == 'train' else val_loader), desc=f'{phase} Phase', unit='batch') as pbar:\n            for inputs, labels in (train_loader if phase == 'train' else val_loader):\n                inputs = inputs.to(device)\n                labels = labels.float().view(-1, 1).to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model_with_attention(inputs)\n                    preds = torch.sigmoid(outputs).round()\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n                pbar.update(1)\n                pbar.set_postfix(loss=running_loss / ((pbar.n + 1) * inputs.size(0)),\n                                 accuracy=running_corrects.double() / ((pbar.n + 1) * inputs.size(0)))\n\n        epoch_loss = running_loss / dataset_sizes[phase]\n        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        if phase == 'val' and epoch_acc > best_acc:\n            best_acc = epoch_acc\n            best_model_wts = model_with_attention.state_dict()\n\n    print()\n\nmodel_with_attention.load_state_dict(best_model_wts)\nprint('Best val Acc: {:4f}'.format(best_acc))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:25:51.816370Z","iopub.execute_input":"2024-05-29T16:25:51.816626Z","iopub.status.idle":"2024-05-29T17:25:56.106811Z","shell.execute_reply.started":"2024-05-29T16:25:51.816596Z","shell.execute_reply":"2024-05-29T17:25:56.105931Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch 1/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [07:48<00:00,  1.60batch/s, accuracy=tensor(0.9857, device='cuda:0', dtype=torch.float64), loss=0.0353]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0353 Acc: 0.9870\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [01:31<00:00,  2.06batch/s, accuracy=tensor(1.9772, device='cuda:0', dtype=torch.float64), loss=0.0226]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0114 Acc: 0.9965\n\nEpoch 2/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [04:51<00:00,  2.57batch/s, accuracy=tensor(0.9937, device='cuda:0', dtype=torch.float64), loss=0.0151]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0151 Acc: 0.9950\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [00:47<00:00,  4.00batch/s, accuracy=tensor(1.9812, device='cuda:0', dtype=torch.float64), loss=0.00921]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0046 Acc: 0.9985\n\nEpoch 3/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [04:47<00:00,  2.61batch/s, accuracy=tensor(0.9941, device='cuda:0', dtype=torch.float64), loss=0.0138] \n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0139 Acc: 0.9955\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [00:47<00:00,  3.95batch/s, accuracy=tensor(1.9815, device='cuda:0', dtype=torch.float64), loss=0.01]   \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0051 Acc: 0.9987\n\nEpoch 4/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [04:48<00:00,  2.60batch/s, accuracy=tensor(0.9947, device='cuda:0', dtype=torch.float64), loss=0.0116]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0117 Acc: 0.9960\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [00:48<00:00,  3.89batch/s, accuracy=tensor(1.9825, device='cuda:0', dtype=torch.float64), loss=0.00496]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0025 Acc: 0.9992\n\nEpoch 5/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [04:50<00:00,  2.58batch/s, accuracy=tensor(0.9940, device='cuda:0', dtype=torch.float64), loss=0.0148] \n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0148 Acc: 0.9953\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [00:48<00:00,  3.91batch/s, accuracy=tensor(1.9673, device='cuda:0', dtype=torch.float64), loss=0.0487]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0246 Acc: 0.9915\n\nEpoch 6/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [04:53<00:00,  2.55batch/s, accuracy=tensor(0.9965, device='cuda:0', dtype=torch.float64), loss=0.00605]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0061 Acc: 0.9978\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [00:49<00:00,  3.82batch/s, accuracy=tensor(1.9825, device='cuda:0', dtype=torch.float64), loss=0.00557]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0028 Acc: 0.9992\n\nEpoch 7/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [04:52<00:00,  2.56batch/s, accuracy=tensor(0.9939, device='cuda:0', dtype=torch.float64), loss=0.0134] \n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0134 Acc: 0.9952\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [00:48<00:00,  3.85batch/s, accuracy=tensor(1.9808, device='cuda:0', dtype=torch.float64), loss=0.0105] \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0053 Acc: 0.9983\n\nEpoch 8/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [04:50<00:00,  2.59batch/s, accuracy=tensor(0.9956, device='cuda:0', dtype=torch.float64), loss=0.00833]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0083 Acc: 0.9969\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [00:48<00:00,  3.91batch/s, accuracy=tensor(1.9775, device='cuda:0', dtype=torch.float64), loss=0.0232]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0117 Acc: 0.9967\n\nEpoch 9/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [04:49<00:00,  2.59batch/s, accuracy=tensor(0.9973, device='cuda:0', dtype=torch.float64), loss=0.00462]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0046 Acc: 0.9986\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [00:47<00:00,  3.94batch/s, accuracy=tensor(1.9798, device='cuda:0', dtype=torch.float64), loss=0.0127] \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0064 Acc: 0.9978\n\nEpoch 10/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [04:47<00:00,  2.60batch/s, accuracy=tensor(0.9919, device='cuda:0', dtype=torch.float64), loss=0.0221] \n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0222 Acc: 0.9932\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 188/188 [00:47<00:00,  3.93batch/s, accuracy=tensor(1.9808, device='cuda:0', dtype=torch.float64), loss=0.00848]","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0043 Acc: 0.9983\n\nBest val Acc: 0.999167\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_curve\nfrom torchvision import models, transforms, datasets\n\n# Ensure the classifier model is correctly defined and loaded\n# Assuming 'classifier_model' is the model you want to evaluate\n# model_with_attention should be already defined and trained\n\n# Define evaluation function\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    true_labels = []\n    predictions = []\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(data_loader, desc='Evaluating', leave=False):\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            probs = torch.sigmoid(outputs).cpu().numpy()\n            true_labels.extend(labels.numpy())\n            predictions.extend(probs)\n\n    return np.array(true_labels), np.array(predictions)\n\n# Define functions to calculate APCER, BPCER, EER, and accuracy\ndef calculate_apcer(true_labels, predictions, fixed_bpcer):\n    fpr, tpr, _ = roc_curve(true_labels, predictions, pos_label=1)\n    fpr_target = fixed_bpcer\n    closest_fpr_index = np.argmin(np.abs(fpr - fpr_target))\n    apcer = 1 - tpr[closest_fpr_index]\n    return apcer\n\ndef calculate_bpcer(true_labels, predictions, fixed_apcer):\n    fpr, tpr, _ = roc_curve(true_labels, predictions, pos_label=1)\n    tpr_target = 1 - fixed_apcer\n    closest_tpr_index = np.argmin(np.abs(tpr - tpr_target))\n    bpcer = fpr[closest_tpr_index]\n    return bpcer\n\ndef calculate_eer(true_labels, predictions):\n    fpr, tpr, _ = roc_curve(true_labels, predictions, pos_label=1)\n    eer_index = np.argmin(np.abs(fpr - (1 - tpr)))\n    eer = fpr[eer_index]\n    return eer\n\ndef calculate_accuracy(true_labels, predictions):\n    binary_predictions = (predictions > 0.5).astype(int)\n    accuracy = np.mean(true_labels == binary_predictions)\n    return accuracy\n\n# Compute metrics for a dataset\ndef compute_metrics_for_dataset(model, data_loader, device, fixed_bpcer_values, fixed_apcer_values):\n    true_labels, predictions = evaluate_model(model, data_loader, device)\n    metrics = {\n        'APCER': {bpcer: calculate_apcer(true_labels, predictions, bpcer) for bpcer in fixed_bpcer_values},\n        'BPCER': {apcer: calculate_bpcer(true_labels, predictions, apcer) for apcer in fixed_apcer_values},\n        'EER': calculate_eer(true_labels, predictions),\n        'Accuracy': calculate_accuracy(true_labels, predictions)\n    }\n    return metrics\n\n# Define fixed values for APCER and BPCER calculations\nfixed_bpcer_values = [0.01, 0.1, 0.2]\nfixed_apcer_values = [0.01, 0.1, 0.2]\n\n# Create a DataFrame to store results\nresults_df = pd.DataFrame(columns=['Dataset', 'APCER_0.01', 'APCER_0.1', 'APCER_0.2',\n                                   'BPCER_0.01', 'BPCER_0.1', 'BPCER_0.2', 'EER', 'Accuracy'])\n\n# Loaders dictionary\nloaders = {\n    'FaceMorpher': fm_loader,\n    'MIPGAN_I': mg1_loader,\n    'MIPGAN_II': mg2_loader,\n    'OpenCV': oc_loader,\n    'Webmorph': wm_loader\n}\n\n# Iterate through the loaders and compute metrics for each dataset\nfor dataset_name, loader in loaders.items():\n    metrics = compute_metrics_for_dataset(model_with_attention, loader, device, fixed_bpcer_values, fixed_apcer_values)\n    \n    # Extract APCER and BPCER values for each fixed threshold\n    apcer_0_01 = metrics['APCER'][0.01]\n    apcer_0_1 = metrics['APCER'][0.1]\n    apcer_0_2 = metrics['APCER'][0.2]\n    \n    bpcer_0_01 = metrics['BPCER'][0.01]\n    bpcer_0_1 = metrics['BPCER'][0.1]\n    bpcer_0_2 = metrics['BPCER'][0.2]\n    \n    # Create a DataFrame for the current dataset metrics\n    df = pd.DataFrame({'Dataset': [dataset_name],\n                       'APCER_0.01': [apcer_0_01],\n                       'APCER_0.1': [apcer_0_1],\n                       'APCER_0.2': [apcer_0_2],\n                       'BPCER_0.01': [bpcer_0_01],\n                       'BPCER_0.1': [bpcer_0_1],\n                       'BPCER_0.2': [bpcer_0_2],\n                       'EER': [metrics['EER']],\n                       'Accuracy': [metrics['Accuracy']]})\n\n    # Concatenate the current dataset DataFrame with the results_df\n    results_df = pd.concat([results_df, df], ignore_index=True)\n\nresults_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T17:25:56.108199Z","iopub.execute_input":"2024-05-29T17:25:56.108477Z","iopub.status.idle":"2024-05-29T17:29:43.140822Z","shell.execute_reply.started":"2024-05-29T17:25:56.108453Z","shell.execute_reply":"2024-05-29T17:29:43.139989Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1828690681.py:109: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  results_df = pd.concat([results_df, df], ignore_index=True)\n                                                           \r","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"       Dataset  APCER_0.01  APCER_0.1  APCER_0.2  BPCER_0.01  BPCER_0.1  \\\n0  FaceMorpher    0.784314   0.450980   0.235294    0.582000   0.325000   \n1     MIPGAN_I    0.759804   0.514706   0.392157    0.956000   0.675000   \n2    MIPGAN_II    0.661765   0.397059   0.279412    0.842843   0.457457   \n3       OpenCV    0.401961   0.142157   0.083333    0.552846   0.135163   \n4     Webmorph    0.460784   0.151961   0.088235    0.568000   0.152000   \n\n   BPCER_0.2       EER  Accuracy  \n0   0.232000  0.217000  0.815739  \n1   0.501000  0.313000  0.815739  \n2   0.300300  0.243243  0.815592  \n3   0.059959  0.119919  0.813361  \n4   0.080000  0.134000  0.694102  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>APCER_0.01</th>\n      <th>APCER_0.1</th>\n      <th>APCER_0.2</th>\n      <th>BPCER_0.01</th>\n      <th>BPCER_0.1</th>\n      <th>BPCER_0.2</th>\n      <th>EER</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FaceMorpher</td>\n      <td>0.784314</td>\n      <td>0.450980</td>\n      <td>0.235294</td>\n      <td>0.582000</td>\n      <td>0.325000</td>\n      <td>0.232000</td>\n      <td>0.217000</td>\n      <td>0.815739</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MIPGAN_I</td>\n      <td>0.759804</td>\n      <td>0.514706</td>\n      <td>0.392157</td>\n      <td>0.956000</td>\n      <td>0.675000</td>\n      <td>0.501000</td>\n      <td>0.313000</td>\n      <td>0.815739</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MIPGAN_II</td>\n      <td>0.661765</td>\n      <td>0.397059</td>\n      <td>0.279412</td>\n      <td>0.842843</td>\n      <td>0.457457</td>\n      <td>0.300300</td>\n      <td>0.243243</td>\n      <td>0.815592</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>OpenCV</td>\n      <td>0.401961</td>\n      <td>0.142157</td>\n      <td>0.083333</td>\n      <td>0.552846</td>\n      <td>0.135163</td>\n      <td>0.059959</td>\n      <td>0.119919</td>\n      <td>0.813361</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Webmorph</td>\n      <td>0.460784</td>\n      <td>0.151961</td>\n      <td>0.088235</td>\n      <td>0.568000</td>\n      <td>0.152000</td>\n      <td>0.080000</td>\n      <td>0.134000</td>\n      <td>0.694102</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\n# Assuming `model_with_attention` is your trained model\n# and `device` is already defined (e.g., `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`)\n\n# Function to calculate the confusion matrix\ndef calculate_confusion_matrix(model, data_loader, device):\n    true_labels = []\n    predicted_labels = []\n\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in tqdm(data_loader, desc='Calculating Confusion Matrix', leave=False):\n            inputs = inputs.to(device)\n            labels = labels.float().view(-1, 1).to(device)\n\n            outputs = model(inputs)\n            preds = (torch.sigmoid(outputs) > 0.5).float()\n\n            true_labels.extend(labels.cpu().numpy())\n            predicted_labels.extend(preds.cpu().numpy())\n\n    true_labels = np.concatenate(true_labels)\n    predicted_labels = np.concatenate(predicted_labels)\n\n    confusion_mat = confusion_matrix(true_labels, predicted_labels)\n    return confusion_mat\n\n# Assuming `loaders` is a dictionary of data loaders defined earlier\ndata_loaders = [fm_loader, mg1_loader, mg2_loader, oc_loader, wm_loader]\ndataset_names = [\"FaceMorpher\", \"MIPGAN_I\", \"MIPGAN_II\", \"OpenCV\", \"Webmorph\"]\n\nconfusion_matrices = []\n\n# Calculate and store confusion matrices for each dataset\nfor loader_idx, data_loader in enumerate(data_loaders):\n    dataset_name = dataset_names[loader_idx]\n    print(f\"Calculating confusion matrix for dataset: {dataset_name}\")\n\n    confusion_mat = calculate_confusion_matrix(model_with_attention, data_loader, device)\n    print(confusion_mat)\n    confusion_matrices.append(confusion_mat)\n\n# Print the confusion matrices\n# for dataset_name, confusion_mat in zip(dataset_names, confusion_matrices):\n#     print(f\"\\nConfusion Matrix - {dataset_name}:\")\n#     print(confusion_mat)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T17:29:43.142434Z","iopub.execute_input":"2024-05-29T17:29:43.142867Z","iopub.status.idle":"2024-05-29T17:32:21.408428Z","shell.execute_reply.started":"2024-05-29T17:29:43.142829Z","shell.execute_reply":"2024-05-29T17:32:21.407457Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Calculating confusion matrix for dataset: FaceMorpher\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"[[1000    0]\n [ 177   27]]\nCalculating confusion matrix for dataset: MIPGAN_I\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"[[1000    0]\n [ 177   27]]\nCalculating confusion matrix for dataset: MIPGAN_II\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"[[999   0]\n [177  27]]\nCalculating confusion matrix for dataset: OpenCV\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"[[984   0]\n [177  27]]\nCalculating confusion matrix for dataset: Webmorph\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"[[500   0]\n [177  27]]\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}]},{"cell_type":"code","source":"# import torch\n# from torchvision import datasets\n# from torch.utils.data import DataLoader\n\n# # Function to count images per class in a dataset\n# def count_images_per_class(dataset):\n#     class_counts = {}\n#     for _, label in dataset.samples:\n#         class_name = dataset.classes[label]\n#         if class_name in class_counts:\n#             class_counts[class_name] += 1\n#         else:\n#             class_counts[class_name] = 1\n#     return class_counts\n\n# # List of all datasets\n# datasets_list = [\n#     ('Train', train_dataset),\n#     ('Validation', val_dataset),\n#     ('FM', fm_dataset),\n#     ('MG1', mg1_dataset),\n#     ('MG2', mg2_dataset),\n#     ('OC', oc_dataset),\n#     ('WM', wm_dataset),\n# ]\n\n# # Print number of images per class for each dataset\n# for name, dataset in datasets_list:\n#     class_counts = count_images_per_class(dataset)\n#     print(f'{name} Dataset:')\n#     for class_name, count in class_counts.items():\n#         print(f'  Class: {class_name}, Number of images: {count}')\n#     print()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T17:32:21.409663Z","iopub.execute_input":"2024-05-29T17:32:21.410005Z","iopub.status.idle":"2024-05-29T17:32:21.414627Z","shell.execute_reply.started":"2024-05-29T17:32:21.409955Z","shell.execute_reply":"2024-05-29T17:32:21.413805Z"},"trusted":true},"execution_count":39,"outputs":[]}]}