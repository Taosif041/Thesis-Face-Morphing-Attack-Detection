{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8271362,"sourceType":"datasetVersion","datasetId":4910963},{"sourceId":8290075,"sourceType":"datasetVersion","datasetId":4924493}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_curve\nimport pandas as pd\n# from apex import amp  # for mixed precision training\nfrom torchvision import models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-22T10:34:10.604007Z","iopub.execute_input":"2024-05-22T10:34:10.604366Z","iopub.status.idle":"2024-05-22T10:34:17.236343Z","shell.execute_reply.started":"2024-05-22T10:34:10.604338Z","shell.execute_reply":"2024-05-22T10:34:17.235379Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Define transforms for data augmentation and normalization\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:34:17.238095Z","iopub.execute_input":"2024-05-22T10:34:17.238633Z","iopub.status.idle":"2024-05-22T10:34:17.245921Z","shell.execute_reply.started":"2024-05-22T10:34:17.238608Z","shell.execute_reply":"2024-05-22T10:34:17.245015Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Set the paths to your training and validation directories\ntrain_dir = '/kaggle/input/morph-splitted/train'\nval_dir = '/kaggle/input/morph-splitted/val'\ntest_dir = '/kaggle/input/morph-splitted/test'","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:34:17.247156Z","iopub.execute_input":"2024-05-22T10:34:17.247472Z","iopub.status.idle":"2024-05-22T10:34:17.257002Z","shell.execute_reply.started":"2024-05-22T10:34:17.247444Z","shell.execute_reply":"2024-05-22T10:34:17.256171Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"image_datasets = {\n    'train': datasets.ImageFolder(train_dir, data_transforms['train']),\n    'val': datasets.ImageFolder(val_dir, data_transforms['val']),\n    'test': datasets.ImageFolder(test_dir, data_transforms['test'])\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:34:17.258920Z","iopub.execute_input":"2024-05-22T10:34:17.259216Z","iopub.status.idle":"2024-05-22T10:34:55.459532Z","shell.execute_reply.started":"2024-05-22T10:34:17.259193Z","shell.execute_reply":"2024-05-22T10:34:55.458502Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Create data loaders\ndataloaders = {\n    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=4),\n    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=4),\n    'test': DataLoader(image_datasets['test'], batch_size=32, shuffle=False, num_workers=4)\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:34:55.460991Z","iopub.execute_input":"2024-05-22T10:34:55.461600Z","iopub.status.idle":"2024-05-22T10:34:55.466966Z","shell.execute_reply.started":"2024-05-22T10:34:55.461571Z","shell.execute_reply":"2024-05-22T10:34:55.466011Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:34:55.468256Z","iopub.execute_input":"2024-05-22T10:34:55.468523Z","iopub.status.idle":"2024-05-22T10:34:55.501870Z","shell.execute_reply.started":"2024-05-22T10:34:55.468500Z","shell.execute_reply":"2024-05-22T10:34:55.500975Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Initialize the model, loss function, and optimizer\nclass BinaryVGG16(nn.Module):\n    def __init__(self):\n        super(BinaryVGG16, self).__init__()\n        self.features = models.vgg16(pretrained=True).features\n        for param in self.features.parameters():\n            param.requires_grad = False\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 1)  # Output layer with 1 neuron for binary classification\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Initialize the model\nmodel = BinaryVGG16().to(device)\ncriterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)  # Only train the classifier","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:34:55.503082Z","iopub.execute_input":"2024-05-22T10:34:55.503677Z","iopub.status.idle":"2024-05-22T10:35:02.055312Z","shell.execute_reply.started":"2024-05-22T10:34:55.503652Z","shell.execute_reply":"2024-05-22T10:35:02.054508Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 169MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 10\nbest_model_wts = model.state_dict()\nbest_acc = 0.0\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:35:02.056415Z","iopub.execute_input":"2024-05-22T10:35:02.056700Z","iopub.status.idle":"2024-05-22T10:35:02.062504Z","shell.execute_reply.started":"2024-05-22T10:35:02.056676Z","shell.execute_reply":"2024-05-22T10:35:02.061575Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs}')\n    print('-' * 10)\n\n    # Each epoch has a training and validation phase\n    for phase in ['train', 'val']:\n        if phase == 'train':\n            model.train()  # Set model to training mode\n        else:\n            model.eval()  # Set model to evaluate mode\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over data with progress bar\n        with tqdm(total=len(dataloaders[phase]), desc=f'{phase} Phase', unit='batch') as pbar:\n            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # Zero the parameter gradients\n                optimizer.zero_grad()\n\n                # Forward pass\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    preds = torch.sigmoid(outputs).round()\n                    loss = criterion(outputs, labels.unsqueeze(1).float())\n\n                    # Backward pass and optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.unsqueeze(1))\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_postfix(loss=running_loss / ((pbar.n + 1) * inputs.size(0)),\n                                 accuracy=running_corrects.double() / ((pbar.n + 1) * inputs.size(0)))\n\n        epoch_loss = running_loss / dataset_sizes[phase]\n        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        # Deep copy the model\n        if phase == 'val' and epoch_acc > best_acc:\n            best_acc = epoch_acc\n            best_model_wts = model.state_dict()\n\n    print()\n\n# Load best model weights\nmodel.load_state_dict(best_model_wts)\nprint('Best val Acc: {:4f}'.format(best_acc))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:36:32.171508Z","iopub.execute_input":"2024-05-22T10:36:32.172202Z","iopub.status.idle":"2024-05-22T10:54:40.248371Z","shell.execute_reply.started":"2024-05-22T10:36:32.172169Z","shell.execute_reply":"2024-05-22T10:54:40.247368Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:30<00:00,  8.32batch/s, accuracy=tensor(0.9372, device='cuda:0', dtype=torch.float64), loss=0.188]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.1886 Acc: 0.9384\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:28<00:00,  8.69batch/s, accuracy=tensor(0.9869, device='cuda:0', dtype=torch.float64), loss=0.0239] \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0240 Acc: 0.9909\n\nEpoch 2/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:24<00:00,  8.90batch/s, accuracy=tensor(0.9636, device='cuda:0', dtype=torch.float64), loss=0.105] \n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.1051 Acc: 0.9649\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:23<00:00, 10.70batch/s, accuracy=tensor(0.9783, device='cuda:0', dtype=torch.float64), loss=0.069]  \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0693 Acc: 0.9823\n\nEpoch 3/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:24<00:00,  8.88batch/s, accuracy=tensor(0.9643, device='cuda:0', dtype=torch.float64), loss=0.113]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.1132 Acc: 0.9656\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:23<00:00, 10.75batch/s, accuracy=tensor(0.9909, device='cuda:0', dtype=torch.float64), loss=0.0149] \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0150 Acc: 0.9949\n\nEpoch 4/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:24<00:00,  8.88batch/s, accuracy=tensor(0.9695, device='cuda:0', dtype=torch.float64), loss=0.0951]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0952 Acc: 0.9708\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:23<00:00, 10.77batch/s, accuracy=tensor(0.9802, device='cuda:0', dtype=torch.float64), loss=0.0514]  \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0516 Acc: 0.9841\n\nEpoch 5/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:24<00:00,  8.89batch/s, accuracy=tensor(0.9684, device='cuda:0', dtype=torch.float64), loss=0.11]  \n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.1100 Acc: 0.9697\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:23<00:00, 10.73batch/s, accuracy=tensor(0.9853, device='cuda:0', dtype=torch.float64), loss=0.0364]  \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0365 Acc: 0.9893\n\nEpoch 6/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:24<00:00,  8.90batch/s, accuracy=tensor(0.9708, device='cuda:0', dtype=torch.float64), loss=0.109] \n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.1087 Acc: 0.9721\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:23<00:00, 10.62batch/s, accuracy=tensor(0.9884, device='cuda:0', dtype=torch.float64), loss=0.0261]  \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0262 Acc: 0.9924\n\nEpoch 7/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:24<00:00,  8.90batch/s, accuracy=tensor(0.9740, device='cuda:0', dtype=torch.float64), loss=0.09]  \n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0901 Acc: 0.9752\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:23<00:00, 10.73batch/s, accuracy=tensor(0.9920, device='cuda:0', dtype=torch.float64), loss=0.0136]  \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0137 Acc: 0.9960\n\nEpoch 8/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:24<00:00,  8.91batch/s, accuracy=tensor(0.9737, device='cuda:0', dtype=torch.float64), loss=0.0864]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0865 Acc: 0.9750\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:23<00:00, 10.70batch/s, accuracy=tensor(0.9817, device='cuda:0', dtype=torch.float64), loss=0.0539]  \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0542 Acc: 0.9856\n\nEpoch 9/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:24<00:00,  8.88batch/s, accuracy=tensor(0.9733, device='cuda:0', dtype=torch.float64), loss=0.0924]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.0926 Acc: 0.9746\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:23<00:00, 10.78batch/s, accuracy=tensor(0.9700, device='cuda:0', dtype=torch.float64), loss=0.0926] \n","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0930 Acc: 0.9739\n\nEpoch 10/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"train Phase: 100%|██████████| 750/750 [01:24<00:00,  8.88batch/s, accuracy=tensor(0.9711, device='cuda:0', dtype=torch.float64), loss=0.112]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.1123 Acc: 0.9724\n","output_type":"stream"},{"name":"stderr","text":"val Phase: 100%|██████████| 250/250 [00:23<00:00, 10.73batch/s, accuracy=tensor(0.9856, device='cuda:0', dtype=torch.float64), loss=0.0371]  ","output_type":"stream"},{"name":"stdout","text":"val Loss: 0.0373 Acc: 0.9895\n\nBest val Acc: 0.996000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test the model\nmodel.eval()\nrunning_loss = 0.0\nrunning_corrects = 0\n\nwith torch.no_grad():\n    for inputs, labels in dataloaders['test']:\n        inputs = inputs.to(device)\n        labels = labels.float().view(-1, 1).to(device)\n\n        outputs = model(inputs)\n        preds = torch.sigmoid(outputs) > 0.5\n        loss = criterion(outputs, labels)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n\ntest_loss = running_loss / len(image_datasets['test'])\ntest_acc = running_corrects.double() / len(image_datasets['test'])\n\nprint(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:54:40.250421Z","iopub.execute_input":"2024-05-22T10:54:40.250747Z","iopub.status.idle":"2024-05-22T10:55:10.656170Z","shell.execute_reply.started":"2024-05-22T10:54:40.250700Z","shell.execute_reply":"2024-05-22T10:55:10.655080Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Test Loss: 0.0370 Acc: 0.9888\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\n# Function to create data loaders\ndef create_data_loader(data_dir, transform, batch_size):\n    dataset = datasets.ImageFolder(data_dir, transform)\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    return data_loader\n\n# Define the transforms for the datasets\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nbatch_size = 512\n\n# Create data loaders for each dataset\nfm_loader = create_data_loader('/kaggle/input/mad-benchmark/FaceMorpher', transform, batch_size)\nmg1_loader = create_data_loader('/kaggle/input/mad-benchmark/MIPGAN_I', transform, batch_size)\nmg2_loader = create_data_loader('/kaggle/input/mad-benchmark/MIPGAN_II', transform, batch_size)\noc_loader = create_data_loader('/kaggle/input/mad-benchmark/OpenCV', transform, batch_size)\nwm_loader = create_data_loader('/kaggle/input/mad-benchmark/Webmorph', transform, batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:55:10.657486Z","iopub.execute_input":"2024-05-22T10:55:10.657815Z","iopub.status.idle":"2024-05-22T10:55:51.404465Z","shell.execute_reply.started":"2024-05-22T10:55:10.657787Z","shell.execute_reply":"2024-05-22T10:55:51.403624Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data_loaders = [fm_loader, mg1_loader, mg2_loader, oc_loader, wm_loader]\nresults = []\nlosses = []\n\n# Evaluate the model on each dataset\ncriterion = nn.BCEWithLogitsLoss()\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:55:51.406507Z","iopub.execute_input":"2024-05-22T10:55:51.406819Z","iopub.status.idle":"2024-05-22T10:55:51.415531Z","shell.execute_reply.started":"2024-05-22T10:55:51.406794Z","shell.execute_reply":"2024-05-22T10:55:51.414685Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"BinaryVGG16(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"for loader_idx, data_loader in enumerate(data_loaders):\n    running_loss = 0.0\n    running_corrects = 0\n    total_samples = 0\n\n    dataset_name = [\"FaceMorpher\", \"MIPGAN_I\", \"MIPGAN_II\", \"OpenCV\", \"Webmorph\"][loader_idx]\n    print(f\"Evaluating dataset: {dataset_name}\")\n\n    for inputs, labels in tqdm(data_loader, desc=f\"Processing {dataset_name}\", leave=False):\n        inputs = inputs.to(device)\n        labels = labels.float().view(-1, 1).to(device)\n\n        outputs = model(inputs)\n        preds = torch.sigmoid(outputs) > 0.5\n        loss = criterion(outputs, labels)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        total_samples += inputs.size(0)\n\n    test_loss = running_loss / total_samples\n    test_accuracy = running_corrects.double() / total_samples\n\n    results.append(test_accuracy.item())\n    losses.append(test_loss)\n\n    print(f\"{dataset_name} - Loss: {test_loss:.4f} Acc: {test_accuracy:.4f}\")\n\n# Print the final results\nprint(\"\\nFinal Results:\")\nnames = [\"FaceMorpher\", \"MIPGAN_I\", \"MIPGAN_II\", \"OpenCV\", \"Webmorph\"]\nfor name, accuracy in zip(names, results):\n    print(f\"{name}: {accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:55:51.416574Z","iopub.execute_input":"2024-05-22T10:55:51.416854Z","iopub.status.idle":"2024-05-22T10:57:59.129483Z","shell.execute_reply.started":"2024-05-22T10:55:51.416831Z","shell.execute_reply":"2024-05-22T10:57:59.128485Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Evaluating dataset: FaceMorpher\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"FaceMorpher - Loss: 2.7654 Acc: 0.4452\nEvaluating dataset: MIPGAN_I\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"MIPGAN_I - Loss: 25.7922 Acc: 0.1694\nEvaluating dataset: MIPGAN_II\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"MIPGAN_II - Loss: 25.8337 Acc: 0.1696\nEvaluating dataset: OpenCV\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"OpenCV - Loss: 19.6401 Acc: 0.1717\nEvaluating dataset: Webmorph\n","output_type":"stream"},{"name":"stderr","text":"                                                                  ","output_type":"stream"},{"name":"stdout","text":"Webmorph - Loss: 19.0276 Acc: 0.2898\n\nFinal Results:\nFaceMorpher: 0.4452\nMIPGAN_I: 0.1694\nMIPGAN_II: 0.1696\nOpenCV: 0.1717\nWebmorph: 0.2898\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Evaluation Metrics**","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_curve\n\ndef calculate_apcer(true_labels, predictions, fixed_bpcer):\n    \"\"\"Calculate APCER at a fixed BPCER.\"\"\"\n    fpr, tpr, thresholds = roc_curve(true_labels, predictions, pos_label=1)\n    fpr_target = fixed_bpcer\n    closest_fpr_index = np.argmin(np.abs(fpr - fpr_target))\n    apcer = 1 - tpr[closest_fpr_index]\n    return apcer\n\ndef calculate_bpcer(true_labels, predictions, fixed_apcer):\n    \"\"\"Calculate BPCER at a fixed APCER.\"\"\"\n    fpr, tpr, thresholds = roc_curve(true_labels, predictions, pos_label=1)\n    tpr_target = 1 - fixed_apcer\n    closest_tpr_index = np.argmin(np.abs(tpr - tpr_target))\n    bpcer = fpr[closest_tpr_index]\n    return bpcer\n\ndef calculate_eer(true_labels, predictions):\n    \"\"\"Calculate EER.\"\"\"\n    fpr, tpr, thresholds = roc_curve(true_labels, predictions, pos_label=1)\n    frr = 1 - tpr\n    eer_index = np.argmin(np.abs(fpr - frr))\n    eer = fpr[eer_index]\n    return eer\n\n# Define datasets and model predictions\ndatasets = [fm_loader, mg1_loader, mg2_loader, oc_loader, wm_loader]\nnames = [\"FaceMorpher\", \"MIPGAN_I\", \"MIPGAN_II\", \"OpenCV\", \"Webmorph\"]\nfixed_bpcer_values = [0.01, 0.1, 0.2]\nfixed_apcer_values = [0.01, 0.1, 0.2]\nall_results = []\n\n# Move the model to the GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()  # Ensure the model is in evaluation mode\n\n# Iterate over each dataset\nfor dataset, name in zip(datasets, names):\n    print(f\"Evaluating model on dataset: {name}\")\n    \n    # Predictions and true labels\n    all_predictions = []\n    all_true_labels = []\n    for inputs, labels in dataset:\n        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the GPU\n        with torch.no_grad():  # Disable gradient computation\n            predictions = model(inputs)\n        all_predictions.append(predictions.detach().cpu().numpy())\n        all_true_labels.append(labels.cpu().numpy())\n    predictions = np.concatenate(all_predictions)\n    true_labels = np.concatenate(all_true_labels)\n\n    # Calculate metrics for each fixed BPCER\n    for fixed_bpcer in fixed_bpcer_values:\n        print(f\"Calculating metrics for fixed BPCER: {fixed_bpcer}\")\n        apcer = calculate_apcer(true_labels, predictions, fixed_bpcer)\n        result = {\n            \"Dataset\": name,\n            \"Fixed BPCER\": f\"{fixed_bpcer * 100:.1f}%\",\n            \"APCER\": apcer\n        }\n        all_results.append(result)\n    \n    # Calculate metrics for each fixed APCER\n    for fixed_apcer in fixed_apcer_values:\n        print(f\"Calculating metrics for fixed APCER: {fixed_apcer}\")\n        bpcer = calculate_bpcer(true_labels, predictions, fixed_apcer)\n        result = {\n            \"Dataset\": name,\n            \"Fixed APCER\": f\"{fixed_apcer * 100:.1f}%\",\n            \"BPCER\": bpcer\n        }\n        all_results.append(result)\n\n    # Calculate EER\n    eer = calculate_eer(true_labels, predictions)\n    result = {\n        \"Dataset\": name,\n        \"EER\": eer\n    }\n    all_results.append(result)\n\n# Convert the results to a Pandas DataFrame\ndf_results = pd.DataFrame(all_results)\n\n# Display the DataFrame\nprint(df_results)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:57:59.131335Z","iopub.execute_input":"2024-05-22T10:57:59.131637Z","iopub.status.idle":"2024-05-22T10:59:36.841107Z","shell.execute_reply.started":"2024-05-22T10:57:59.131610Z","shell.execute_reply":"2024-05-22T10:59:36.839927Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Evaluating model on dataset: FaceMorpher\nCalculating metrics for fixed BPCER: 0.01\nCalculating metrics for fixed BPCER: 0.1\nCalculating metrics for fixed BPCER: 0.2\nCalculating metrics for fixed APCER: 0.01\nCalculating metrics for fixed APCER: 0.1\nCalculating metrics for fixed APCER: 0.2\nEvaluating model on dataset: MIPGAN_I\nCalculating metrics for fixed BPCER: 0.01\nCalculating metrics for fixed BPCER: 0.1\nCalculating metrics for fixed BPCER: 0.2\nCalculating metrics for fixed APCER: 0.01\nCalculating metrics for fixed APCER: 0.1\nCalculating metrics for fixed APCER: 0.2\nEvaluating model on dataset: MIPGAN_II\nCalculating metrics for fixed BPCER: 0.01\nCalculating metrics for fixed BPCER: 0.1\nCalculating metrics for fixed BPCER: 0.2\nCalculating metrics for fixed APCER: 0.01\nCalculating metrics for fixed APCER: 0.1\nCalculating metrics for fixed APCER: 0.2\nEvaluating model on dataset: OpenCV\nCalculating metrics for fixed BPCER: 0.01\nCalculating metrics for fixed BPCER: 0.1\nCalculating metrics for fixed BPCER: 0.2\nCalculating metrics for fixed APCER: 0.01\nCalculating metrics for fixed APCER: 0.1\nCalculating metrics for fixed APCER: 0.2\nEvaluating model on dataset: Webmorph\nCalculating metrics for fixed BPCER: 0.01\nCalculating metrics for fixed BPCER: 0.1\nCalculating metrics for fixed BPCER: 0.2\nCalculating metrics for fixed APCER: 0.01\nCalculating metrics for fixed APCER: 0.1\nCalculating metrics for fixed APCER: 0.2\n        Dataset Fixed BPCER     APCER Fixed APCER     BPCER       EER\n0   FaceMorpher        1.0%  0.002000         NaN       NaN       NaN\n1   FaceMorpher       10.0%  0.002000         NaN       NaN       NaN\n2   FaceMorpher       20.0%  0.002000         NaN       NaN       NaN\n3   FaceMorpher         NaN       NaN        1.0%  0.000000       NaN\n4   FaceMorpher         NaN       NaN       10.0%  0.000000       NaN\n5   FaceMorpher         NaN       NaN       20.0%  0.000000       NaN\n6   FaceMorpher         NaN       NaN         NaN       NaN  0.004902\n7      MIPGAN_I        1.0%  0.966000         NaN       NaN       NaN\n8      MIPGAN_I       10.0%  0.821000         NaN       NaN       NaN\n9      MIPGAN_I       20.0%  0.687000         NaN       NaN       NaN\n10     MIPGAN_I         NaN       NaN        1.0%  0.916667       NaN\n11     MIPGAN_I         NaN       NaN       10.0%  0.740196       NaN\n12     MIPGAN_I         NaN       NaN       20.0%  0.598039       NaN\n13     MIPGAN_I         NaN       NaN         NaN       NaN  0.406863\n14    MIPGAN_II        1.0%  0.969970         NaN       NaN       NaN\n15    MIPGAN_II       10.0%  0.824825         NaN       NaN       NaN\n16    MIPGAN_II       20.0%  0.691692         NaN       NaN       NaN\n17    MIPGAN_II         NaN       NaN        1.0%  0.931373       NaN\n18    MIPGAN_II         NaN       NaN       10.0%  0.740196       NaN\n19    MIPGAN_II         NaN       NaN       20.0%  0.573529       NaN\n20    MIPGAN_II         NaN       NaN         NaN       NaN  0.401961\n21       OpenCV        1.0%  0.762195         NaN       NaN       NaN\n22       OpenCV       10.0%  0.391260         NaN       NaN       NaN\n23       OpenCV       20.0%  0.232724         NaN       NaN       NaN\n24       OpenCV         NaN       NaN        1.0%  0.754902       NaN\n25       OpenCV         NaN       NaN       10.0%  0.392157       NaN\n26       OpenCV         NaN       NaN       20.0%  0.220588       NaN\n27       OpenCV         NaN       NaN         NaN       NaN  0.215686\n28     Webmorph        1.0%  0.884000         NaN       NaN       NaN\n29     Webmorph       10.0%  0.592000         NaN       NaN       NaN\n30     Webmorph       20.0%  0.412000         NaN       NaN       NaN\n31     Webmorph         NaN       NaN        1.0%  0.848039       NaN\n32     Webmorph         NaN       NaN       10.0%  0.514706       NaN\n33     Webmorph         NaN       NaN       20.0%  0.372549       NaN\n34     Webmorph         NaN       NaN         NaN       NaN  0.264706\n","output_type":"stream"}]}]}